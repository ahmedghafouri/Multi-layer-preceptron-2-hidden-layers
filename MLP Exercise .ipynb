{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The implementation of the MLP algorithm will be in the mlp.py file but run through this notebook\n",
    "#### This code will use the MNIST Handwritten Digit Classification Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP \n",
    "import numpy as np\n",
    "\n",
    "# This is to reload all changed modules every time before executing a new line.\n",
    "# https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (10000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# loading the MNIST datatset\n",
    "import pickle, gzip\n",
    "\n",
    "f = gzip.open('mnist.pkl.gz','rb')\n",
    "tset, vset, teset = pickle.load(f, encoding='latin1')\n",
    "print(tset[0].shape, vset[0].shape, teset[0].shape)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # To install: pip install matplotlib\n",
    "\n",
    "# visualise some examples from the dataset \n",
    "fig, ax = plt.subplots(2,5)\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    im_idx = np.argwhere(teset[1] == i)[0]\n",
    "    plottable_image = np.reshape(teset[0][im_idx], (28, 28))\n",
    "    ax.imshow(plottable_image, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 9000 images for training and 1000 for testing \n",
    "\n",
    "# Just use the first 9000 images for training \n",
    "tread = 9000\n",
    "train_in = tset[0][:tread,:]\n",
    "\n",
    "# This is a little bit of work -- 1 of N encoding\n",
    "train_tgt = np.zeros((tread,10))\n",
    "for i in range(tread):\n",
    "    train_tgt[i,tset[1][i]] = 1\n",
    "\n",
    "# and use 1000 images for testing\n",
    "teread = 1000\n",
    "test_in = teset[0][:teread,:]\n",
    "test_tgt = np.zeros((teread,10))\n",
    "for i in range(teread):\n",
    "    test_tgt[i,teset[1][i]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise the MLP classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first and second hidden layers to have 5 neurons each.\n",
    "sizes = [784,5,5,10] # 784 is the number of pixels of the images and 10 is the number of classes \n",
    "classifier = MLP(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Error:  4117.6080419244745\n",
      "Iteration:  100  Error:  4046.2617172938217\n",
      "Iteration:  200  Error:  4042.0029740608347\n",
      "Iteration:  300  Error:  4037.3965165083214\n",
      "Iteration:  400  Error:  4030.1819396617757\n",
      "Iteration:  500  Error:  4017.571654349916\n",
      "Iteration:  600  Error:  3993.1614139404605\n",
      "Iteration:  700  Error:  3940.0879531330797\n",
      "Iteration:  800  Error:  3825.087084636112\n",
      "Iteration:  900  Error:  3692.816363797018\n"
     ]
    }
   ],
   "source": [
    "# TODO: open the mlp.py file and implement self.forwardPass and self.train methods\n",
    "# for now, let's keep the learning rate and the number of iterations unchanged  \n",
    "classifier.train(train_in, train_tgt, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is:\n",
      "[[ 85.   0.  37.  78. 103.  72.  71.  60.  64.  83.]\n",
      " [  0. 125.  41.   7.   2.   4.   4.  16.  11.   3.]\n",
      " [  0.   1.  26.  10.   0.   2.   6.   0.   9.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   1.   1.   0.   4.   0.   3.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.  12.  12.   4.   8.   6.  19.   5.   5.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "The accuracy is  25.6\n"
     ]
    }
   ],
   "source": [
    "#evaluate our model on the testing set \n",
    "# and show the confusion matrix and the accuracy\n",
    "classifier.evaluate(test_in, test_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Error:  4102.675834023828\n",
      "Iteration:  100  Error:  481.0678474791383\n",
      "Iteration:  200  Error:  209.30731419077895\n",
      "Iteration:  300  Error:  120.97961676182538\n",
      "Iteration:  400  Error:  87.06166611461765\n",
      "Iteration:  500  Error:  70.34299707496079\n",
      "The confusion matrix is:\n",
      "[[ 78.   0.   1.   0.   0.   0.   4.   0.   0.   0.]\n",
      " [  0. 125.   0.   0.   0.   0.   0.   1.   1.   0.]\n",
      " [  0.   1. 105.   3.   1.   0.   4.   3.   0.   0.]\n",
      " [  0.   0.   1.  97.   0.   4.   0.   2.   2.   0.]\n",
      " [  0.   0.   0.   0.  99.   1.   3.   0.   1.   3.]\n",
      " [  4.   0.   0.   5.   0.  78.   1.   0.   4.   2.]\n",
      " [  2.   0.   1.   0.   2.   0.  75.   0.   1.   0.]\n",
      " [  0.   0.   5.   1.   0.   0.   0.  89.   0.   3.]\n",
      " [  0.   0.   2.   1.   1.   3.   0.   0.  77.   2.]\n",
      " [  1.   0.   1.   0.   7.   1.   0.   4.   3.  84.]]\n",
      "The accuracy is  90.7\n"
     ]
    }
   ],
   "source": [
    "best_sizes = [784,30,30,10]\n",
    "best_beta = 1\n",
    "best_momentum =0.9\n",
    "best_lr =6 # best learning rate\n",
    "best_niterations = 600\n",
    "best_classifier = MLP(sizes = best_sizes, beta=best_beta, momentum=best_momentum)\n",
    "best_classifier.train(train_in, train_tgt, best_lr, best_niterations)\n",
    "best_classifier.evaluate(test_in, test_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the following code to save the best parameters and \n",
    "# the weights of the network that achieves the desired accuracy\n",
    "best_parameters = {\n",
    "    'sizes': best_sizes,\n",
    "    'beta': best_beta,\n",
    "    'momentum': best_momentum,\n",
    "    'lr': best_lr,\n",
    "    'niterations': best_niterations,\n",
    "    'weights_1': best_classifier.weights1,\n",
    "    'weights_2': best_classifier.weights2,\n",
    "    'weights_3': best_classifier.weights3,\n",
    "}\n",
    "\n",
    "with open('best_classifier.pkl', 'wb') as handle:\n",
    "    pickle.dump(best_parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
